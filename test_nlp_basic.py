#!/usr/bin/env python3
"""
Test basique du moteur NLP LexLang
Script pour vÃ©rifier que le moteur fonctionne correctement
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

from core.nlp_engine import NLPProcessor, Token
import json

def test_basic_functionality():
    """Test des fonctionnalitÃ©s de base"""
    print("ğŸ”§ Initialisation du processeur NLP...")
    
    try:
        processor = NLPProcessor()
        print("âœ… Processeur initialisÃ© avec succÃ¨s")
    except Exception as e:
        print(f"âŒ Erreur d'initialisation: {e}")
        return False
    
    return True

def test_text_normalization():
    """Test de la normalisation de texte"""
    print("\nğŸ“ Test de normalisation de texte...")
    
    processor = NLPProcessor()
    
    test_cases = [
        ("Bonjour, Comment ALLEZ-vous ?", "bonjour, comment allez-vous ?"),
        ("  Espaces   multiples  ", "espaces multiples"),
        ("Ã€ccÃ©nts Ã©t Ã§aractÃ¨res spÃ©ciaux", "Ã ccÃ©nts Ã©t Ã§aractÃ¨res spÃ©ciaux"),
        ("MAJUSCULES ET minuscules", "majuscules et minuscules")
    ]
    
    for input_text, expected in test_cases:
        result = processor.normalize_text(input_text)
        status = "âœ…" if result == expected else "âŒ"
        print(f"{status} '{input_text}' â†’ '{result}'")
        if result != expected:
            print(f"   Attendu: '{expected}'")
    
    return True

def test_tokenization():
    """Test de la tokenisation"""
    print("\nğŸ”¤ Test de tokenisation...")
    
    processor = NLPProcessor()
    
    test_texts = [
        "Bonjour tout le monde !",
        "Salam aleikum, nanga def ?",
        "Comment Ã§a va aujourd'hui ?",
        "Les langues africaines sont belles.",
        "123 nombres et mots-composÃ©s"
    ]
    
    for text in test_texts:
        tokens = processor.tokenize(text)
        print(f"ğŸ“„ '{text}'")
        print(f"   â†’ Tokens: {tokens}")
        print(f"   â†’ Nombre: {len(tokens)}")
    
    return True

def test_pos_tagging():
    """Test de l'Ã©tiquetage morpho-syntaxique"""
    print("\nğŸ·ï¸  Test d'Ã©tiquetage POS...")
    
    processor = NLPProcessor()
    
    test_words = [
        "bonjour", "le", "chat", "mange", "rapidement",
        "dans", "maison", "et", "je", "trÃ¨s",
        "tion", "ment", "able", "er"
    ]
    
    for word in test_words:
        pos = processor.simple_pos_tag(word)
        print(f"   {word:12} â†’ {pos}")
    
    return True

def test_full_text_processing():
    """Test du traitement complet de texte"""
    print("\nğŸ” Test de traitement complet...")
    
    processor = NLPProcessor()
    
    test_texts = [
        "Bonjour, comment allez-vous aujourd'hui ?",
        "Salam aleikum ! Nanga def ak sa famille ?",
        "Les langues africaines sont trÃ¨s riches et diversifiÃ©es.",
        "L'intelligence artificielle peut aider Ã  prÃ©server nos langues ancestrales."
    ]
    
    for i, text in enumerate(test_texts, 1):
        print(f"\n--- Test {i} ---")
        print(f"Texte: {text}")
        
        try:
            result = processor.process_text(text)
            print(f"âœ… Traitement rÃ©ussi:")
            print(f"   â€¢ Tokens trouvÃ©s: {result['tokens_count']}")
            print(f"   â€¢ Tokens uniques: {result['unique_tokens']}")
            print(f"   â€¢ N-grammes gÃ©nÃ©rÃ©s: {result['ngrams_generated']}")
            
            # Afficher quelques tokens traitÃ©s
            if result['tokens']:
                print("   â€¢ Premiers tokens:")
                for token_data in result['tokens'][:3]:
                    print(f"     - {token_data['text']} [{token_data['pos']}] (freq: {token_data['frequency']})")
        
        except Exception as e:
            print(f"âŒ Erreur de traitement: {e}")
            return False
    
    return True

def test_ngrams_generation():
    """Test de gÃ©nÃ©ration de n-grammes"""
    print("\nğŸ“Š Test de gÃ©nÃ©ration de n-grammes...")
    
    processor = NLPProcessor()
    
    # Traiter un texte d'abord
    text = "Bonjour tout le monde, comment allez-vous aujourd'hui ?"
    processor.process_text(text)
    
    # VÃ©rifier les n-grammes gÃ©nÃ©rÃ©s
    for n in [2, 3, 4]:
        ngrams = processor.lexical_db.n_grams[n]
        print(f"   {n}-grammes: {len(ngrams)} gÃ©nÃ©rÃ©s")
        if ngrams:
            # Afficher les 3 premiers
            items = list(ngrams.items())[:3]
            for ngram, freq in items:
                print(f"     â€¢ '{ngram}' (freq: {freq})")
    
    return True

def test_database_operations():
    """Test des opÃ©rations de base de donnÃ©es"""
    print("\nğŸ’¾ Test des opÃ©rations de base de donnÃ©es...")
    
    processor = NLPProcessor()
    
    # Ajouter quelques tokens manuellement
    test_token = Token(
        text="test",
        lemma="test",
        pos="NOUN",
        features={"length": "4"},
        frequency=5,
        contexts=["Ceci est un test"]
    )
    
    processor.lexical_db.add_token(test_token)
    
    # Tester la recherche
    found_token = processor.lexical_db.get_token("test")
    if found_token:
        print("âœ… Token ajoutÃ© et retrouvÃ© avec succÃ¨s")
        print(f"   â€¢ Texte: {found_token.text}")
        print(f"   â€¢ POS: {found_token.pos}")
        print(f"   â€¢ FrÃ©quence: {found_token.frequency}")
    else:
        print("âŒ Token non retrouvÃ©")
        return False
    
    return True

def test_statistics():
    """Test des statistiques"""
    print("\nğŸ“ˆ Test des statistiques...")
    
    processor = NLPProcessor()
    
    # Traiter quelques textes
    texts = [
        "Bonjour comment allez-vous ?",
        "Salam aleikum nanga def ?",
        "Bonsoir tout le monde !"
    ]
    
    for text in texts:
        processor.process_text(text)
    
    # Obtenir les statistiques
    try:
        stats = processor.get_statistics()
        print("âœ… Statistiques gÃ©nÃ©rÃ©es:")
        print(f"   â€¢ Total tokens: {stats['total_tokens']}")
        print(f"   â€¢ Lemmes uniques: {stats['unique_lemmas']}")
        print(f"   â€¢ Distribution POS: {stats['pos_distribution']}")
        if stats['most_frequent']:
            print("   â€¢ Mots les plus frÃ©quents:")
            for word, freq in stats['most_frequent'][:3]:
                print(f"     - {word}: {freq}")
    except Exception as e:
        print(f"âŒ Erreur dans les statistiques: {e}")
        return False
    
    return True

def test_save_load():
    """Test de sauvegarde/chargement"""
    print("\nğŸ’¾ Test de sauvegarde/chargement...")
    
    processor = NLPProcessor()
    
    # Traiter du texte
    processor.process_text("Test de sauvegarde avec quelques mots.")
    
    # Sauvegarder
    try:
        processor.save_database("test_database.pkl")
        print("âœ… Sauvegarde rÃ©ussie")
    except Exception as e:
        print(f"âŒ Erreur de sauvegarde: {e}")
        return False
    
    # CrÃ©er un nouveau processeur et charger
    try:
        new_processor = NLPProcessor()
        new_processor.load_database("test_database.pkl")
        print("âœ… Chargement rÃ©ussi")
        
        # VÃ©rifier que les donnÃ©es sont lÃ 
        if len(new_processor.lexical_db.tokens) > 0:
            print(f"   â€¢ {len(new_processor.lexical_db.tokens)} tokens chargÃ©s")
        else:
            print("âŒ Aucun token chargÃ©")
            return False
            
    except Exception as e:
        print(f"âŒ Erreur de chargement: {e}")
        return False
    
    # Nettoyer
    try:
        os.remove("test_database.pkl")
        print("âœ… Fichier de test nettoyÃ©")
    except:
        pass
    
    return True

def run_all_tests():
    """ExÃ©cute tous les tests"""
    print("ğŸ§ª TESTS DU MOTEUR NLP LEXLANG")
    print("=" * 50)
    
    tests = [
        ("FonctionnalitÃ© de base", test_basic_functionality),
        ("Normalisation de texte", test_text_normalization),
        ("Tokenisation", test_tokenization),
        ("Ã‰tiquetage POS", test_pos_tagging),
        ("Traitement complet", test_full_text_processing),
        ("GÃ©nÃ©ration n-grammes", test_ngrams_generation),
        ("OpÃ©rations BDD", test_database_operations),
        ("Statistiques", test_statistics),
        ("Sauvegarde/Chargement", test_save_load)
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        print(f"\nğŸ§ª {test_name}...")
        print("-" * 30)
        
        try:
            if test_func():
                passed += 1
                print(f"âœ… {test_name}: RÃ‰USSI")
            else:
                print(f"âŒ {test_name}: Ã‰CHEC")
        except Exception as e:
            print(f"âŒ {test_name}: ERREUR - {e}")
    
    print("\n" + "=" * 50)
    print(f"ğŸ“Š RÃ‰SULTATS: {passed}/{total} tests rÃ©ussis")
    
    if passed == total:
        print("ğŸ‰ TOUS LES TESTS SONT PASSÃ‰S !")
        return True
    else:
        print("âš ï¸  Certains tests ont Ã©chouÃ©")
        return False

if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)
